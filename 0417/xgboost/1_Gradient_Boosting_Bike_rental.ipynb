{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 그레이디언트 부스팅 (Gradient Boosting)","metadata":{}},{"cell_type":"markdown","source":"### 1. 필요한 Python 라이브러리 Import 하기","metadata":{}},{"cell_type":"code","source":"# pandas와 numpy 임포트하기\nimport pandas as pd\nimport numpy as np\n\n# 경고 끄기\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nimport xgboost as xgb\n\nxgb.set_config(verbosity=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_bikes = pd.read_csv(\"bike_rentals_cleaned.csv\")\ndf_bikes.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 데이터를 X와 y로 나눕니다.\nX_bikes = df_bikes.iloc[:, :-1]\ny_bikes = df_bikes.iloc[:, -1]\n\n# train_test_split를 임포트합니다.\nfrom sklearn.model_selection import train_test_split\n\n# 데이터를 훈련 세트와 테스트 세트로 나눕니다.\nX_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. DecisionTree를 활용한 그레이디언트 부스팅 모델 만들기","metadata":{}},{"cell_type":"code","source":"# DecisionTreeRegressor를 임포트합니다.\nfrom sklearn.tree import DecisionTreeRegressor\n\n# DecisionTreeRegressor를 초기화합니다.\n# gradient boosting은 결정 트리를 이용해 높은 정확도를 얻는 거이 아니라 오차를 통해 학습하는 모델을 원하기\n# 때문에 앙상블의 첫번째 트리인 tree_1을 max_depth=2로 결정트리를 초기화\ntree_1 = DecisionTreeRegressor(max_depth=2, random_state=2)\n\n# 훈련 세트에 결정 트리를 훈련합니다.\ntree_1.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 테스트 세트가 아니라 훈련 세트에 대한 예측을 만듭니다.\n# 잔차를 계산하기 위해서 훈련 단계에서 예측과 타깃을 비교해야 하기 때문임\n# 모델의 테스트 단계는 모든 트리를 구성한 후 마지막에 시행\ny_train_pred = tree_1.predict(X_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 잔차를 계산합니다.\ny2_train = y_train - y_train_pred\n\n# 새로운 DecisionTreeRegressor를 초기화합니다.\ntree_2 = DecisionTreeRegressor(max_depth=2, random_state=2)\n\n# 잔차에 모델을 훈련합니다.\ntree_2.fit(X_train, y2_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 훈련 세트에 대한 예측을 만듭니다.\ny2_train_pred = tree_2.predict(X_train)\n\n# 잔차를 계산합니다.\ny3_train = y2_train - y2_train_pred\n\n# 새로운 DecisionTreeRegressor를 초기화합니다.\ntree_3 = DecisionTreeRegressor(max_depth=2, random_state=2)\n\n# 잔차에 모델을 훈련합니다.\ntree_3.fit(X_train, y3_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y1_pred = tree_1.predict(X_test)\n\ny2_pred = tree_2.predict(X_test)\n\ny3_pred = tree_3.predict(X_test)\n\ny_pred = y1_pred + y2_pred + y3_pred\n\n# mean_squared_error를 임포트합니다.\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# 평균 제곱근 오차를 계산합니다.\nMSE(y_test, y_pred) ** 0.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. 사이킷런으로 그레이디언트 부스팅 모델 만들기","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gbr = GradientBoostingRegressor(\n    max_depth=2, n_estimators=3, random_state=2, learning_rate=1.0\n)\n\ngbr.fit(X_train, y_train)\n\n# 테스트 데이터에 대한 예측을 만듭니다.\ny_pred = gbr.predict(X_test)\n\n# 평균 제곱근 오차를 계산합니다.\nMSE(y_test, y_pred) ** 0.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gbr = GradientBoostingRegressor(\n    max_depth=2, n_estimators=30, random_state=2, learning_rate=1.0\n)\ngbr.fit(X_train, y_train)\ny_pred = gbr.predict(X_test)\nMSE(y_test, y_pred) ** 0.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gbr = GradientBoostingRegressor(\n    max_depth=2, n_estimators=300, random_state=2, learning_rate=1.0\n)\ngbr.fit(X_train, y_train)\ny_pred = gbr.predict(X_test)\nMSE(y_test, y_pred) ** 0.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gbr = GradientBoostingRegressor(\n    max_depth=2, n_estimators=300, random_state=2, learning_rate=0.1\n)\ngbr.fit(X_train, y_train)\ny_pred = gbr.predict(X_test)\nMSE(y_test, y_pred) ** 0.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. 그레이디언트 부스팅 매개변수 튜닝\n\n### 4.1 learning_rate","metadata":{}},{"cell_type":"code","source":"learning_rate_values = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 1.0]\nfor value in learning_rate_values:\n    gbr = GradientBoostingRegressor(\n        max_depth=2, n_estimators=300, random_state=2, learning_rate=value\n    )\n    gbr.fit(X_train, y_train)\n    y_pred = gbr.predict(X_test)\n    rmse = MSE(y_test, y_pred) ** 0.5\n    print(\"학습률:\", value, \", 점수:\", rmse)","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nfor est in [30, 300, 3000]:\n    rmse_scores = []\n    for value in learning_rate_values:\n        gbr = GradientBoostingRegressor(\n            max_depth=2, n_estimators=est, random_state=2, learning_rate=value\n        )\n        gbr.fit(X_train, y_train)\n        y_pred = gbr.predict(X_test)\n        rmse = mean_squared_error(y_test, y_pred)\n        rmse_scores.append(rmse)\n    plt.figure(figsize=(10, 3))\n    plt.plot(learning_rate_values, rmse_scores)\n    plt.xlabel(\"learning_rate\")\n    plt.ylabel(\"RMSE\")\n    plt.title(\"Gradient Boosting learning_rate {} trees\".format(est))\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.2 max depth","metadata":{}},{"cell_type":"code","source":"depths = [None, 1, 2, 3, 4]\nfor depth in depths:\n    gbr = GradientBoostingRegressor(max_depth=depth, n_estimators=300, random_state=2)\n    gbr.fit(X_train, y_train)\n    y_pred = gbr.predict(X_test)\n    rmse = MSE(y_test, y_pred) ** 0.5\n    print(\"최대 깊이:\", depth, \", 점수:\", rmse)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gbr.init_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(gbr.estimators_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 3))\nplt.plot(range(1, 301), gbr.train_score_)\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"train_score_\")\nplt.title(\"Train Score\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.3 subsample","metadata":{}},{"cell_type":"code","source":"samples = [1, 0.9, 0.8, 0.7, 0.6, 0.5]\nfor sample in samples:\n    gbr = GradientBoostingRegressor(\n        max_depth=3, n_estimators=300, subsample=sample, random_state=2\n    )\n    gbr.fit(X_train, y_train)\n    y_pred = gbr.predict(X_test)\n    rmse = MSE(y_test, y_pred) ** 0.5\n    print(\"subsample:\", sample, \", 점수:\", rmse)","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 3))\nplt.plot(range(1, 301), gbr.oob_improvement_)\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"oob_improvement_\")\nplt.title(\"OOB Improvement\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.4 RandomizedSearchCV","metadata":{}},{"cell_type":"code","source":"params = {\n    \"subsample\": [0.65, 0.7, 0.75],\n    \"n_estimators\": [100, 200, 300],\n    \"learning_rate\": [0.05, 0.075, 0.1],\n}\n\n# RandomizedSearchCV를 임포트합니다.\nfrom sklearn.model_selection import RandomizedSearchCV\n\ngbr = GradientBoostingRegressor(max_depth=3, random_state=2)\n\n\n# RandomizedSearchCV를 초기화합니다.\nrand_reg = RandomizedSearchCV(\n    gbr,\n    params,\n    n_iter=10,\n    scoring=\"neg_mean_squared_error\",\n    cv=5,\n    n_jobs=-1,\n    random_state=2,\n)\n\n# X_train와 y_train로 rand_reg를 훈련합니다.\nrand_reg.fit(X_train, y_train)\n\n# 최상의 모델을 추출합니다.\nbest_model = rand_reg.best_estimator_\n\n# 최상의 매개변수를 추출합니다.\nbest_params = rand_reg.best_params_\n\n# 최상의 매개변수를 출력합니다.\nprint(\"최상의 매개변수:\", best_params)\n\n# 최상의 점수를 계산합니다.\nbest_score = np.sqrt(-rand_reg.best_score_)\n\n# 최상의 점수를 출력합니다.\nprint(\"훈련 점수: {:.3f}\".format(best_score))\n\n# 테스트 세트에 대한 예측을 만듭니다.\ny_pred = best_model.predict(X_test)\n\n# 평균 제곱근 오차를 계산합니다.\nrmse_test = MSE(y_test, y_pred) ** 0.5\n\n# 평균 제곱근 오차를 출력합니다.\nprint(\"테스트 세트 점수: {:.3f}\".format(rmse_test))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gbr = GradientBoostingRegressor(\n    max_depth=3, n_estimators=1600, subsample=0.75, learning_rate=0.02, random_state=2\n)\ngbr.fit(X_train, y_train)\ny_pred = gbr.predict(X_test)\nMSE(y_test, y_pred) ** 0.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.5 XGBoost","metadata":{}},{"cell_type":"code","source":"# XGBRegressor를 임포트합니다.\nfrom xgboost import XGBRegressor\n\n# XGBRegressor를 초기화합니다.\nxg_reg = XGBRegressor(\n    max_depth=3, n_estimators=1600, eta=0.02, subsample=0.75, random_state=2\n)\n\n# 훈련 세트에서 xg_reg를 훈련합니다.\nxg_reg.fit(X_train, y_train)\n\n# 테스트 세트에 대한 예측을 만듭니다.\ny_pred = xg_reg.predict(X_test)\n\n# 평균 제곱근 오차를 계산합니다.\nMSE(y_test, y_pred) ** 0.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}